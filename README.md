# ğŸ¤ Advanced Speech Synthesis System

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Uma soluÃ§Ã£o avanÃ§ada e completa para sÃ­ntese de fala (Text-to-Speech) e clonagem de voz, construÃ­da com tecnologias de ponta em deep learning.

## âœ¨ CaracterÃ­sticas Principais

### ğŸ¯ Funcionalidades Core
- **SÃ­ntese de Fala de Alta Qualidade**: Modelos TTS state-of-the-art (YourTTS, Tacotron2, FastSpeech2)
- **Clonagem de Voz**: Clone qualquer voz com apenas 3-10 segundos de Ã¡udio
- **Suporte Multi-idiomas**: InglÃªs, Espanhol, FrancÃªs, AlemÃ£o e mais
- **Interface Web Moderna**: Interface Gradio intuitiva e responsiva
- **API REST**: Endpoints para integraÃ§Ã£o em aplicaÃ§Ãµes

### ğŸš€ Melhorias TÃ©cnicas
- **Arquitetura Transformer AvanÃ§ada**: ImplementaÃ§Ã£o prÃ³pria com attention multi-head
- **Sistema de Treinamento Robusto**: Early stopping, gradient clipping, mixed precision
- **Processamento de Texto Inteligente**: NormalizaÃ§Ã£o automÃ¡tica, fonemizaÃ§Ã£o
- **Cache e OtimizaÃ§Ãµes**: Sistema de cache para inferÃªncia rÃ¡pida
- **Monitoramento Completo**: Logs estruturados, mÃ©tricas de performance

### ğŸ”§ Funcionalidades TÃ©cnicas
- **MÃºltiplas Arquiteturas**: YourTTS, Tacotron2, FastSpeech2, VITS
- **Vocoders Neurais**: HiFi-GAN, MelGAN, WaveRNN
- **Suporte a GPU**: CUDA, MPS (Apple Silicon), CPU
- **Treinamento DistribuÃ­do**: Multi-GPU support
- **Reprodutibilidade**: Seeds fixas, deterministic training
- **AvaliaÃ§Ã£o Automatizada**: MÃ©tricas PESQ, STOI, SNR, MOS
- **PrÃ©-processamento Inteligente**: Suporte a mÃºltiplos formatos de dataset
- **Export para ProduÃ§Ã£o**: ONNX, TorchScript, Mobile, QuantizaÃ§Ã£o

## ğŸ†• Novas Funcionalidades

### ğŸ“Š Sistema de AvaliaÃ§Ã£o
- **MÃ©tricas de Qualidade**: PESQ, STOI, SNR, distorÃ§Ã£o espectral
- **Benchmark de Performance**: MediÃ§Ã£o de velocidade de inferÃªncia (RTF)
- **ComparaÃ§Ã£o de Modelos**: AvaliaÃ§Ã£o side-by-side
- **RelatÃ³rios Automatizados**: GeraÃ§Ã£o de relatÃ³rios JSON detalhados

### ğŸ”§ PrÃ©-processamento AvanÃ§ado
- **DetecÃ§Ã£o AutomÃ¡tica de Formato**: LJSpeech, Common Voice, VCTK, genÃ©rico
- **NormalizaÃ§Ã£o de Ãudio**: Reamostragem, normalizaÃ§Ã£o de volume
- **CriaÃ§Ã£o de Espectrogramas**: Mel-spectrograms prÃ©-computados
- **DivisÃ£o Inteligente**: Train/validation/test splits configurÃ¡veis

### ğŸ“¦ Export para ProduÃ§Ã£o
- **MÃºltiplos Formatos**: TorchScript, ONNX, Mobile, Quantizado
- **Pacotes de Deploy**: CriaÃ§Ã£o automÃ¡tica de pacotes completos
- **Scripts de InferÃªncia**: Scripts prontos para produÃ§Ã£o
- **OtimizaÃ§Ã£o de Performance**: QuantizaÃ§Ã£o dinÃ¢mica e estÃ¡tica

## ğŸ“ Estrutura do Projeto

```
Speech_synthesis/
â”œâ”€â”€ ğŸ“ src/                          # CÃ³digo principal
â”‚   â”œâ”€â”€ ğŸ model.py                  # Modelos TTS avanÃ§ados
â”‚   â”œâ”€â”€ ğŸ train.py                  # Sistema de treinamento
â”‚   â”œâ”€â”€ ğŸ data.py                   # Datasets e data loaders
â”‚   â”œâ”€â”€ ğŸ text_processor.py         # Processamento de texto
â”‚   â”œâ”€â”€ ğŸ gradio_interface.py       # Interface web moderna
â”‚   â”œâ”€â”€ ğŸ tts_model.py             # Wrapper para Coqui TTS
â”‚   â”œâ”€â”€ ğŸ speaker_encoder.py        # Encoder para clonagem
â”‚   â”œâ”€â”€ ğŸ logging_config.py         # Sistema de logs
â”‚   â”œâ”€â”€ ğŸ utils.py                  # UtilitÃ¡rios
â”‚   â”œâ”€â”€ ğŸ infer.py                  # InferÃªncia
â”‚   â”œâ”€â”€ ğŸ clone.py                  # Clonagem de voz
â”‚   â”œâ”€â”€ ğŸ vocoder.py               # Vocoders neurais
â”‚   â”œâ”€â”€ ğŸ evaluate.py              # Sistema de avaliaÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ preprocess.py            # PrÃ©-processamento
â”‚   â””â”€â”€ ğŸ export.py                # Export para produÃ§Ã£o
â”œâ”€â”€ ğŸ“ models/                       # Modelos prÃ©-treinados
â”œâ”€â”€ ğŸ“ data/                         # Datasets
â”œâ”€â”€ ğŸ“ notebooks/                    # Jupyter notebooks
â”œâ”€â”€ ğŸ“ tests/                        # Testes automatizados
â”œâ”€â”€ ğŸ“ logs/                         # Logs do sistema
â”œâ”€â”€ ğŸ“ cache/                        # Cache de processamento
â”œâ”€â”€ ğŸ”§ config.yaml                   # ConfiguraÃ§Ã£o principal
â”œâ”€â”€ ğŸ”§ pyproject.toml               # ConfiguraÃ§Ã£o do projeto
â”œâ”€â”€ ğŸ“‹ requirements.txt              # DependÃªncias
â”œâ”€â”€ ğŸš€ main.py                      # Entry point CLI
â””â”€â”€ ğŸ“– README.md                    # Esta documentaÃ§Ã£o
```

## ğŸš€ InstalaÃ§Ã£o RÃ¡pida

### PrÃ©-requisitos
- Python 3.8+ 
- PyTorch 2.0+
- CUDA (opcional, para GPU)

### InstalaÃ§Ã£o das DependÃªncias

```bash
# Clone o repositÃ³rio
git clone <repository-url>
cd Speech_synthesis

# Crie um ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Instale as dependÃªncias
pip install -r requirements.txt

# Instale espeak (necessÃ¡rio para fonemizaÃ§Ã£o)
# Ubuntu/Debian:
sudo apt-get install espeak espeak-data libespeak1 libespeak-dev

# Windows: Baixe e instale do site oficial
# macOS:
brew install espeak
```

### InstalaÃ§Ã£o para Desenvolvimento

```bash
# Instale em modo desenvolvimento
pip install -e .

# Instale dependÃªncias de desenvolvimento
pip install -e ".[dev]"

# Configure pre-commit hooks
pre-commit install
```

## ğŸ’» Uso RÃ¡pido

### Interface Web (Recomendado)

```bash
# Inicie a interface web
python src/gradio_interface.py

# Ou use o script principal
python main.py web
```

Acesse: `http://localhost:7860`

### CLI - SÃ­ntese de Fala

```bash
```bash
# SÃ­ntese bÃ¡sica
python main.py infer \
    --text "OlÃ¡, este Ã© um teste de sÃ­ntese de fala" \
    --output audio_saida.wav

# Com modelo personalizado
python main.py infer \
    --text "Texto para sintetizar" \
    --model models/meu_modelo.pth \
    --config config.yaml \
    --output resultado.wav
```

### CLI - Clonagem de Voz

```bash
# Clone uma voz
python main.py clone \
    --text "Texto para falar com a voz clonada" \
    --reference audio_referencia.wav \
    --output voz_clonada.wav

# Com configuraÃ§Ãµes avanÃ§adas
python main.py clone \
    --text "Seu texto aqui" \
    --reference voz_referencia.wav \
    --similarity-threshold 0.8 \
    --output resultado_clonado.wav
```

### CLI - AvaliaÃ§Ã£o de Modelo

```bash
# Benchmark de performance
python main.py evaluate \
    --benchmark \
    --checkpoint-path models/checkpoint.pt \
    --repetitions 10

# AvaliaÃ§Ã£o em dataset
python main.py evaluate \
    --dataset data/test \
    --checkpoint-path models/checkpoint.pt \
    --output-dir evaluation_results
```

### CLI - PrÃ©-processamento

```bash
# Processar dataset LJSpeech
python main.py preprocess \
    --input-dir data/LJSpeech-1.1 \
    --output-dir data/processed \
    --format ljspeech

# Detectar formato automaticamente
python main.py preprocess \
    --input-dir data/raw_dataset \
    --output-dir data/processed \
    --format auto
```

### CLI - Export para ProduÃ§Ã£o

```bash
# Export TorchScript
python main.py export \
    --model-path models/checkpoint.pt \
    --output exports/model.pt \
    --format torchscript

# Export ONNX
python main.py export \
    --model-path models/checkpoint.pt \
    --output exports/model.onnx \
    --format onnx

# Criar pacote completo de deploy
python main.py export \
    --model-path models/checkpoint.pt \
    --output deployment_package \
    --format package \
    --include-formats torchscript,onnx,mobile
```

### Treinamento de Modelo

```bash
# Prepare seus dados
mkdir -p data/train data/val

# Organize como: audio.wav + audio.txt para cada amostra

# Inicie o treinamento
python main.py train \
    --config config.yaml \
    --output-dir ./outputs \
    --device auto

# Retome treinamento
python main.py train \
    --config config.yaml \
    --resume outputs/checkpoint_epoch_10.pth
```

## âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada

### Arquivo de ConfiguraÃ§Ã£o Principal

O arquivo `config.yaml` controla todos os aspectos do sistema:

```yaml
# ConfiguraÃ§Ã£o do sistema
system:
  device: "auto"  # auto, cpu, cuda, mps
  cache_dir: "./cache"
  log_level: "INFO"

# ConfiguraÃ§Ã£o do modelo
model:
  type: "AdvancedTTS"  # AdvancedTTS, YourTTS, Tacotron2
  params:
    d_model: 512
    num_heads: 8
    num_encoder_layers: 6
    num_decoder_layers: 6

# ConfiguraÃ§Ã£o de dados
data:
  sample_rate: 22050
  n_mel_channels: 80
  normalize_audio: true
  trim_silence: true

# Treinamento
training:
  batch_size: 16
  learning_rate: 0.0003
  epochs: 100
  early_stopping_patience: 20

# Interface web
web:
  title: "Sistema de SÃ­ntese de Fala"
  max_text_length: 1000
  enable_voice_cloning: true
```

### ConfiguraÃ§Ãµes de Performance

```yaml
performance:
  enable_fp16: true          # Mixed precision training
  enable_caching: true       # Cache para inferÃªncia
  cache_size: 1000          # NÃºmero de itens no cache
  batch_inference: true      # InferÃªncia em lotes
```

## ğŸ¯ Exemplos de Uso

### Python API

```python
from src.tts_model import TTSWrapper
from src.speaker_encoder import SpeakerEncoder
from src.text_processor import TextProcessor

# Inicializar componentes
tts = TTSWrapper()
speaker_encoder = SpeakerEncoder()
text_processor = TextProcessor(language="pt")

# SÃ­ntese bÃ¡sica
text = "OlÃ¡, como vocÃª estÃ¡ hoje?"
audio = tts.synthesize(text)
tts.save(audio, "output.wav")

# Clonagem de voz
reference_audio = "voz_referencia.wav"
cloned_audio = tts.synthesize(text, speaker_wav=reference_audio)
tts.save(cloned_audio, "voz_clonada.wav")
```

### Processamento em Lote

```python
import os
from pathlib import Path

# Processar mÃºltiplos textos
texts = [
    "Primeiro texto para sÃ­ntese",
    "Segundo texto para sÃ­ntese", 
    "Terceiro texto para sÃ­ntese"
]

output_dir = Path("outputs")
output_dir.mkdir(exist_ok=True)

for i, text in enumerate(texts):
    audio = tts.synthesize(text)
    output_path = output_dir / f"audio_{i:03d}.wav"
    tts.save(audio, str(output_path))
    print(f"Salvo: {output_path}")
```

## ğŸ”¬ Treinamento Personalizado

### PreparaÃ§Ã£o dos Dados

1. **Estrutura dos Dados**:
```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ audio_001.wav
â”‚   â”œâ”€â”€ audio_001.txt
â”‚   â”œâ”€â”€ audio_002.wav
â”‚   â”œâ”€â”€ audio_002.txt
â”‚   â””â”€â”€ ...
â””â”€â”€ val/
    â”œâ”€â”€ audio_val_001.wav
    â”œâ”€â”€ audio_val_001.txt
    â””â”€â”€ ...
```

2. **Formato dos Arquivos**:
   - **Ãudio**: WAV, 22050 Hz, mono
   - **Texto**: UTF-8, uma frase por arquivo
   - **DuraÃ§Ã£o**: 1-20 segundos recomendado

### Script de Treinamento AvanÃ§ado

```python
from omegaconf import OmegaConf
from src.train import TTSTrainer

# Carregue configuraÃ§Ã£o
config = OmegaConf.load("config.yaml")

# Personalize configuraÃ§Ãµes
config.training.batch_size = 32
config.training.learning_rate = 0.001
config.model.params.d_model = 256

# Inicie treinamento
trainer = TTSTrainer(config)
trainer.train()
```

### Monitoramento do Treinamento

```bash
# TensorBoard
tensorboard --logdir logs/

# Weights & Biases (se configurado)
wandb login
# O treinamento irÃ¡ automaticamente logar mÃ©tricas
```

## ğŸ§ª Testes e Qualidade

### Executar Testes

```bash
# Todos os testes
pytest

# Testes especÃ­ficos
pytest tests/test_model.py
pytest tests/test_data.py

# Com cobertura
pytest --cov=src --cov-report=html
```

### AnÃ¡lise de CÃ³digo

```bash
# Linting
flake8 src/
black src/
mypy src/

# Ou use pre-commit
pre-commit run --all-files
```

## ğŸ“Š MÃ©tricas e AvaliaÃ§Ã£o

### MÃ©tricas de Qualidade de Ãudio
- **MOS (Mean Opinion Score)**: AvaliaÃ§Ã£o subjetiva
- **PESQ**: Qualidade perceptual
- **STOI**: Inteligibilidade
- **Mel Cepstral Distortion**: Similaridade espectral

### MÃ©tricas de Performance
- **Real-time Factor**: Velocidade de sÃ­ntese
- **Memory Usage**: Uso de memÃ³ria
- **GPU Utilization**: UtilizaÃ§Ã£o da GPU

### Exemplo de AvaliaÃ§Ã£o

```python
from src.evaluation import AudioEvaluator

evaluator = AudioEvaluator()

# Avaliar qualidade
scores = evaluator.evaluate_batch(
    generated_audios=["output1.wav", "output2.wav"],
    reference_audios=["ref1.wav", "ref2.wav"]
)

print(f"MOS Score: {scores['mos']:.2f}")
print(f"PESQ Score: {scores['pesq']:.2f}")
```

## ğŸ”§ SoluÃ§Ã£o de Problemas

### Problemas Comuns

1. **Erro de ImportError com torch**:
```bash
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118
```

2. **Erro com espeak**:
```bash
# Linux
sudo apt-get install espeak-ng espeak-ng-data

# macOS
brew install espeak-ng
```

3. **Erro de memÃ³ria GPU**:
```yaml
# No config.yaml
training:
  batch_size: 8  # Reduza o batch size
performance:
  enable_fp16: true  # Use mixed precision
```

4. **Ãudio com ruÃ­do**:
```yaml
# No config.yaml
data:
  normalize_audio: true
  trim_silence: true
audio:
  noise_reduction: true
```

### Debug Mode

```bash
# Execute com logs detalhados
python main.py infer \
    --text "Teste" \
    --output test.wav \
    --log-level DEBUG
```

## ğŸ¤ ContribuiÃ§Ã£o

### Como Contribuir

1. **Fork** o repositÃ³rio
2. **Clone** seu fork
3. **Crie** uma branch para sua feature
4. **Implemente** sua mudanÃ§a
5. **Teste** suas modificaÃ§Ãµes
6. **Commit** com mensagens claras
7. **Push** para sua branch
8. **Abra** um Pull Request

### PadrÃµes de CÃ³digo

- **PEP 8** para formataÃ§Ã£o Python
- **Type hints** obrigatÃ³rios
- **Docstrings** para todas as funÃ§Ãµes pÃºblicas
- **Testes** para novas funcionalidades

### Exemplo de ContribuiÃ§Ã£o

```python
def nova_funcionalidade(texto: str, parametro: int = 10) -> str:
    """
    DescriÃ§Ã£o clara da funÃ§Ã£o.
    
    Args:
        texto: DescriÃ§Ã£o do parÃ¢metro
        parametro: DescriÃ§Ã£o com valor padrÃ£o
        
    Returns:
        DescriÃ§Ã£o do retorno
        
    Example:
        >>> resultado = nova_funcionalidade("teste", 5)
        >>> print(resultado)
    """
    # ImplementaÃ§Ã£o aqui
    return resultado
```

## ğŸ“ˆ Roadmap

### VersÃ£o 1.0 (Atual)
- âœ… Interface Gradio moderna
- âœ… Modelos TTS avanÃ§ados
- âœ… Sistema de clonagem de voz
- âœ… Treinamento robusto
- âœ… Processamento de texto inteligente

### VersÃ£o 1.1 (PrÃ³xima)
- ğŸ”„ API REST completa
- ğŸ”„ Suporte a mais idiomas
- ğŸ”„ Modelos de emoÃ§Ã£o
- ğŸ”„ Streaming de Ã¡udio
- ğŸ”„ Mobile deployment

### VersÃ£o 2.0 (Futuro)
- ğŸ¯ Real-time voice conversion
- ğŸ¯ Multi-modal synthesis
- ğŸ¯ Advanced prosody control
- ğŸ¯ Neural vocoder improvements
- ğŸ¯ Cloud deployment

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ™ Agradecimentos

- **Coqui TTS**: Framework base para TTS
- **Resemblyzer**: Speaker encoder
- **Gradio**: Interface web moderna
- **PyTorch**: Framework de deep learning
- **Comunidade Open Source**: InspiraÃ§Ã£o e suporte

## ğŸ“ Suporte

- **Issues**: [GitHub Issues](../../issues)
- **DocumentaÃ§Ã£o**: Este README e cÃ³digo comentado
- **Exemplos**: Pasta `notebooks/`

---

**Desenvolvido com â¤ï¸ para a comunidade de sÃ­ntese de fala**

*Ãšltima atualizaÃ§Ã£o: Julho 2025*
